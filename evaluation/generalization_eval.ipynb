{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a66b52a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchvision version: 0.24.1+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "# Check versions\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "import torchvision\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355df14f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported GPT2 modules\n"
     ]
    }
   ],
   "source": [
    "# Now try importing transformers GPT2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "print(\"Successfully imported GPT2 modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b012c4",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for Linear Relational Embeddings (LRE)\n",
    "\n",
    "## Repository: /net/scratch2/smallyan/relations_eval\n",
    "\n",
    "This notebook evaluates whether the findings in the repository generalize beyond the original experimental setting.\n",
    "\n",
    "## Evaluation Checklist:\n",
    "- **GT1**: Generalization to a New Model\n",
    "- **GT2**: Generalization to New Data  \n",
    "- **GT3**: Method / Specificity Generalizability\n",
    "\n",
    "## Research Summary\n",
    "This repository investigates how transformer language models represent and decode relational knowledge. The key finding is that for a subset of relations, the highly non-linear decoding procedure can be approximated by a simple linear transformation (LRE) on the subject representation at intermediate layers.\n",
    "\n",
    "**Original Models Used**: GPT-J-6B, GPT-2-XL, LLaMA-13B\n",
    "\n",
    "**Method**: Linear Relational Embedding (LRE)\n",
    "- LRE(s) = βWs + b\n",
    "- W = E[∂F/∂s] (mean Jacobian from n=8 examples)\n",
    "- b = E[F(s,c) - (∂F/∂s)s] (bias term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f301827f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Successfully imported all repository modules\n"
     ]
    }
   ],
   "source": [
    "# Import repository modules\n",
    "import sys\n",
    "repo_path = '/net/scratch2/smallyan/relations_eval'\n",
    "sys.path.insert(0, repo_path)\n",
    "\n",
    "from src import models, data, functional\n",
    "from src.operators import JacobianIclMeanEstimator\n",
    "from src.utils import experiment_utils\n",
    "from src.data import RelationSample\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "print(\"Successfully imported all repository modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac259e0",
   "metadata": {},
   "source": [
    "## GT1: Generalization to a New Model\n",
    "\n",
    "**Criterion**: The newly proposed neuron-level finding is predictable on a **new model** not used in the original work.\n",
    "\n",
    "**New Model**: GPT-2 Medium (original study used GPT-J-6B, GPT-2-XL, LLaMA-13B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd56235",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--gpt2-medium/.no_exist/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/adapter_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 Medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--gpt2-medium/.no_exist/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/adapter_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT2LMHeadModel\n",
      "Layers: 24\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 Medium (NOT used in original study)\n",
    "import transformers\n",
    "\n",
    "print('Loading GPT-2 Medium...')\n",
    "model_gpt2_medium = transformers.AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "model_gpt2_medium.to(device)\n",
    "model_gpt2_medium.eval()\n",
    "\n",
    "tokenizer_gpt2_medium = transformers.AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer_gpt2_medium.pad_token = tokenizer_gpt2_medium.eos_token\n",
    "\n",
    "mt_new = models.ModelAndTokenizer(model_gpt2_medium, tokenizer_gpt2_medium)\n",
    "print(f'Model: {type(model_gpt2_medium).__name__}')\n",
    "print(f'Layers: {model_gpt2_medium.config.n_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4d8334",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: country capital city\n",
      "Samples: 24\n",
      "Prompt template: The capital city of {} is\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and test LRE on GPT-2 Medium\n",
    "dataset = data.load_dataset()\n",
    "\n",
    "# Test on 'country capital city' relation\n",
    "relation = dataset.filter(relation_names=['country capital city'])[0]\n",
    "print(f'Relation: {relation.name}')\n",
    "print(f'Samples: {len(relation.samples)}')\n",
    "print(f'Prompt template: {relation.prompt_templates[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120ddf60",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5\n",
      "  China -> Beijing\n",
      "  Japan -> Tokyo\n",
      "  Italy -> Rome\n",
      "  Brazil -> Bras\\u00edlia\n",
      "  Turkey -> Ankara\n",
      "\n",
      "Test samples: 19\n",
      "  South Korea -> Seoul\n",
      "  Colombia -> Bogot\\u00e1\n",
      "  Saudi Arabia -> Riyadh\n",
      "  France -> Paris\n",
      "  Mexico -> Mexico City\n"
     ]
    }
   ],
   "source": [
    "# Set seed and split the data\n",
    "experiment_utils.set_seed(12345)\n",
    "train, test = relation.split(5)\n",
    "\n",
    "print(f\"Training samples: {len(train.samples)}\")\n",
    "for s in train.samples:\n",
    "    print(f\"  {s.subject} -> {s.object}\")\n",
    "\n",
    "print(f\"\\nTest samples: {len(test.samples)}\")\n",
    "for s in test.samples[:5]:\n",
    "    print(f\"  {s.subject} -> {s.object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75204d94",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRE operator created for layer 8 with beta=2.5\n"
     ]
    }
   ],
   "source": [
    "# Create LRE estimator for GPT-2 Medium\n",
    "# Using layer 8 (middle layer for 24-layer model) and beta=2.5 as in the original paper\n",
    "layer = 8\n",
    "beta = 2.5\n",
    "\n",
    "estimator = JacobianIclMeanEstimator(mt=mt_new, h_layer=layer, beta=beta)\n",
    "operator = estimator(relation.set(samples=train.samples))\n",
    "\n",
    "print(f\"LRE operator created for layer {layer} with beta={beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546631af",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered test samples: 3 (from 19)\n"
     ]
    }
   ],
   "source": [
    "# Filter test samples based on model's ability to predict them\n",
    "test_filtered = functional.filter_relation_samples_based_on_provided_fewshots(\n",
    "    mt=mt_new, test_relation=test, prompt_template=operator.prompt_template, batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"Filtered test samples: {len(test_filtered.samples)} (from {len(test.samples)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a85d359",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT1: Testing LRE on GPT-2 Medium (new model not in original study)\n",
      "\n",
      "Trial Examples:\n",
      "------------------------------------------------------------\n",
      "Trial 1: Saudi Arabia -> Expected: Riyadh\n",
      "         Predicted: \"Man\" ✗ FAIL\n",
      "\n",
      "Trial 2: South Korea -> Expected: Seoul\n",
      "         Predicted: \"Seoul\" ✓ PASS\n",
      "\n",
      "Trial 3: United States -> Expected: Washington D.C.\n",
      "         Predicted: \"Washington\" ✓ PASS\n",
      "\n",
      "------------------------------------------------------------\n",
      "GT1 Result: 2/3 successful trials\n",
      "GT1 PASS: At least one successful example\n"
     ]
    }
   ],
   "source": [
    "# Test LRE on the filtered samples - up to 3 trials for GT1\n",
    "gt1_results = []\n",
    "correct = 0\n",
    "\n",
    "print(\"GT1: Testing LRE on GPT-2 Medium (new model not in original study)\\n\")\n",
    "print(\"Trial Examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, sample in enumerate(test_filtered.samples[:3]):  # Up to 3 trials\n",
    "    predictions = operator(subject=sample.subject).predictions\n",
    "    predicted_token = predictions[0].token.strip()\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    status = \"✓ PASS\" if known_flag else \"✗ FAIL\"\n",
    "    print(f'Trial {i+1}: {sample.subject} -> Expected: {sample.object}')\n",
    "    print(f'         Predicted: \"{predicted_token}\" {status}')\n",
    "    print()\n",
    "    \n",
    "    gt1_results.append({\n",
    "        'subject': sample.subject,\n",
    "        'expected': sample.object,\n",
    "        'predicted': predicted_token,\n",
    "        'success': known_flag\n",
    "    })\n",
    "    correct += known_flag\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f'GT1 Result: {correct}/{len(gt1_results)} successful trials')\n",
    "gt1_pass = correct >= 1\n",
    "print(f'GT1 {\"PASS\" if gt1_pass else \"FAIL\"}: {\"At least one successful example\" if gt1_pass else \"No successful examples\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02431d1d",
   "metadata": {},
   "source": [
    "## GT2: Generalization to New Data\n",
    "\n",
    "**Criterion**: The newly proposed neuron-level finding is predictable on **new data instances** not appearing in the original dataset.\n",
    "\n",
    "**New Data**: Testing on countries not in the original 24-country dataset (Poland, Sweden, Norway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3be9ceb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original countries in dataset (24):\n",
      "['United States', 'Canada', 'Mexico', 'Brazil', 'Argentina', 'Chile', 'Peru', 'Colombia', 'Venezuela', 'Spain', 'France', 'Germany', 'Italy', 'Russia', 'China', 'Japan', 'South Korea', 'India', 'Pakistan', 'Nigeria', 'Egypt', 'Saudi Arabia', 'Turkey', 'Australia']\n"
     ]
    }
   ],
   "source": [
    "# Check which countries are in the original dataset\n",
    "original_countries = [s.subject for s in relation.samples]\n",
    "print(f'Original countries in dataset ({len(original_countries)}):')\n",
    "print(original_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5343985",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT2: Testing LRE on new data instances (not in original dataset)\n",
      "\n",
      "Verifying these countries are NOT in original dataset:\n",
      "  Poland: NOT in original ✓\n",
      "  Sweden: NOT in original ✓\n",
      "  Norway: NOT in original ✓\n",
      "\n",
      "Trial Examples:\n",
      "------------------------------------------------------------\n",
      "Trial 1: Poland -> Expected: Warsaw\n",
      "         Predicted: \"Warsaw\" ✓ PASS\n",
      "\n",
      "Trial 2: Sweden -> Expected: Stockholm\n",
      "         Predicted: \"Stockholm\" ✓ PASS\n",
      "\n",
      "Trial 3: Norway -> Expected: Oslo\n",
      "         Predicted: \"Oslo\" ✓ PASS\n",
      "\n",
      "------------------------------------------------------------\n",
      "GT2 Result: 3/3 successful trials\n",
      "GT2 PASS: At least one successful example\n"
     ]
    }
   ],
   "source": [
    "# Test on NEW data instances not in original dataset\n",
    "# These are countries NOT in the original 24-country dataset\n",
    "\n",
    "new_samples = [\n",
    "    RelationSample(subject='Poland', object='Warsaw'),\n",
    "    RelationSample(subject='Sweden', object='Stockholm'),\n",
    "    RelationSample(subject='Norway', object='Oslo'),\n",
    "]\n",
    "\n",
    "print(\"GT2: Testing LRE on new data instances (not in original dataset)\\n\")\n",
    "print(\"Verifying these countries are NOT in original dataset:\")\n",
    "for sample in new_samples:\n",
    "    in_original = sample.subject in original_countries\n",
    "    print(f\"  {sample.subject}: {'IN original (invalid!)' if in_original else 'NOT in original ✓'}\")\n",
    "\n",
    "print(\"\\nTrial Examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "gt2_results = []\n",
    "correct_gt2 = 0\n",
    "\n",
    "for i, sample in enumerate(new_samples[:3]):  # Up to 3 trials\n",
    "    predictions = operator(subject=sample.subject).predictions\n",
    "    predicted_token = predictions[0].token.strip()\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    status = \"✓ PASS\" if known_flag else \"✗ FAIL\"\n",
    "    print(f'Trial {i+1}: {sample.subject} -> Expected: {sample.object}')\n",
    "    print(f'         Predicted: \"{predicted_token}\" {status}')\n",
    "    print()\n",
    "    \n",
    "    gt2_results.append({\n",
    "        'subject': sample.subject,\n",
    "        'expected': sample.object,\n",
    "        'predicted': predicted_token,\n",
    "        'success': known_flag\n",
    "    })\n",
    "    correct_gt2 += known_flag\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f'GT2 Result: {correct_gt2}/{len(gt2_results)} successful trials')\n",
    "gt2_pass = correct_gt2 >= 1\n",
    "print(f'GT2 {\"PASS\" if gt2_pass else \"FAIL\"}: {\"At least one successful example\" if gt2_pass else \"No successful examples\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5664",
   "metadata": {},
   "source": [
    "## GT3: Method / Specificity Generalizability\n",
    "\n",
    "**Criterion**: If the work proposes a **new method**, evaluate if it can be applied to **another similar task**.\n",
    "\n",
    "**New Method**: Linear Relational Embedding (LRE) - Jacobian-based linear approximation\n",
    "\n",
    "**Test**: Apply LRE method to different relation types (factual, commonsense, linguistic) - up to 3 similar tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd42900",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT3: Testing LRE method on different relation types\n",
      "\n",
      "======================================================================\n",
      "Task 1: Word Sentiment (Commonsense Relation)\n",
      "======================================================================\n",
      "Relation: word sentiment\n",
      "Samples: 60\n",
      "Prompt template: The sentiment of '{}' is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered test samples: 9\n",
      "  blessed -> positive: Predicted=\"positive\" ✓\n",
      "  blissful -> positive: Predicted=\"positive\" ✓\n",
      "  cheerful -> positive: Predicted=\"positive\" ✓\n",
      "  delighted -> positive: Predicted=\"positive\" ✓\n",
      "  despairing -> negative: Predicted=\"positive\" ✗\n",
      "\n",
      "Task 1 Result: 4/5 correct\n"
     ]
    }
   ],
   "source": [
    "# Test LRE method on different relation types (up to 3 similar tasks)\n",
    "# Task 1: Word sentiment (commonsense relation)\n",
    "\n",
    "print(\"GT3: Testing LRE method on different relation types\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Task 1: Word Sentiment (Commonsense Relation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "relation2 = dataset.filter(relation_names=['word sentiment'])[0]\n",
    "print(f'Relation: {relation2.name}')\n",
    "print(f'Samples: {len(relation2.samples)}')\n",
    "print(f'Prompt template: {relation2.prompt_templates[0]}')\n",
    "\n",
    "experiment_utils.set_seed(12345)\n",
    "train2, test2 = relation2.split(5)\n",
    "\n",
    "estimator2 = JacobianIclMeanEstimator(mt=mt_new, h_layer=layer, beta=beta)\n",
    "operator2 = estimator2(relation2.set(samples=train2.samples))\n",
    "\n",
    "test2_filtered = functional.filter_relation_samples_based_on_provided_fewshots(\n",
    "    mt=mt_new, test_relation=test2, prompt_template=operator2.prompt_template, batch_size=4\n",
    ")\n",
    "\n",
    "print(f'\\nFiltered test samples: {len(test2_filtered.samples)}')\n",
    "\n",
    "correct_task1 = 0\n",
    "for sample in test2_filtered.samples[:5]:\n",
    "    predictions = operator2(subject=sample.subject).predictions\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    status = \"✓\" if known_flag else \"✗\"\n",
    "    print(f'  {sample.subject} -> {sample.object}: Predicted=\"{predictions[0].token.strip()}\" {status}')\n",
    "    correct_task1 += known_flag\n",
    "\n",
    "print(f'\\nTask 1 Result: {correct_task1}/5 correct')\n",
    "task1_pass = correct_task1 >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9551112b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Task 2: Adjective Antonym (Linguistic Relation)\n",
      "======================================================================\n",
      "Relation: adjective antonym\n",
      "Samples: 100\n",
      "Prompt template: The opposite of {} is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered test samples: 2\n",
      "  inhale -> exhale: Predicted=\"move\" ✗\n",
      "  inhale -> exhale: Predicted=\"move\" ✗\n",
      "\n",
      "Task 2 Result: 0/5 correct\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Adjective antonym (linguistic relation)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Task 2: Adjective Antonym (Linguistic Relation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "relation3 = dataset.filter(relation_names=['adjective antonym'])[0]\n",
    "print(f'Relation: {relation3.name}')\n",
    "print(f'Samples: {len(relation3.samples)}')\n",
    "print(f'Prompt template: {relation3.prompt_templates[0]}')\n",
    "\n",
    "experiment_utils.set_seed(12345)\n",
    "train3, test3 = relation3.split(5)\n",
    "\n",
    "estimator3 = JacobianIclMeanEstimator(mt=mt_new, h_layer=layer, beta=beta)\n",
    "operator3 = estimator3(relation3.set(samples=train3.samples))\n",
    "\n",
    "test3_filtered = functional.filter_relation_samples_based_on_provided_fewshots(\n",
    "    mt=mt_new, test_relation=test3, prompt_template=operator3.prompt_template, batch_size=4\n",
    ")\n",
    "\n",
    "print(f'\\nFiltered test samples: {len(test3_filtered.samples)}')\n",
    "\n",
    "correct_task2 = 0\n",
    "for sample in test3_filtered.samples[:5]:\n",
    "    predictions = operator3(subject=sample.subject).predictions\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    status = \"✓\" if known_flag else \"✗\"\n",
    "    print(f'  {sample.subject} -> {sample.object}: Predicted=\"{predictions[0].token.strip()}\" {status}')\n",
    "    correct_task2 += known_flag\n",
    "\n",
    "print(f'\\nTask 2 Result: {correct_task2}/5 correct')\n",
    "task2_pass = correct_task2 >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574923cb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (People in {} speak)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Task 3: Country Language (Factual Relation)\n",
      "======================================================================\n",
      "Relation: country language\n",
      "Samples: 24\n",
      "Prompt template: People in {} speak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered test samples: 3\n",
      "  Saudi Arabia -> Arabic: Predicted=\"Turkish\" ✗\n",
      "  South Korea -> Korean: Predicted=\"Korean\" ✓\n",
      "  United States -> English: Predicted=\"English\" ✓\n",
      "\n",
      "Task 3 Result: 2/5 correct\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Country language (factual relation - different from country capital)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Task 3: Country Language (Factual Relation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "relation4 = dataset.filter(relation_names=['country language'])[0]\n",
    "print(f'Relation: {relation4.name}')\n",
    "print(f'Samples: {len(relation4.samples)}')\n",
    "print(f'Prompt template: {relation4.prompt_templates[0]}')\n",
    "\n",
    "experiment_utils.set_seed(12345)\n",
    "train4, test4 = relation4.split(5)\n",
    "\n",
    "estimator4 = JacobianIclMeanEstimator(mt=mt_new, h_layer=layer, beta=beta)\n",
    "operator4 = estimator4(relation4.set(samples=train4.samples))\n",
    "\n",
    "test4_filtered = functional.filter_relation_samples_based_on_provided_fewshots(\n",
    "    mt=mt_new, test_relation=test4, prompt_template=operator4.prompt_template, batch_size=4\n",
    ")\n",
    "\n",
    "print(f'\\nFiltered test samples: {len(test4_filtered.samples)}')\n",
    "\n",
    "correct_task3 = 0\n",
    "for sample in test4_filtered.samples[:5]:\n",
    "    predictions = operator4(subject=sample.subject).predictions\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    status = \"✓\" if known_flag else \"✗\"\n",
    "    print(f'  {sample.subject} -> {sample.object}: Predicted=\"{predictions[0].token.strip()}\" {status}')\n",
    "    correct_task3 += known_flag\n",
    "\n",
    "print(f'\\nTask 3 Result: {correct_task3}/5 correct')\n",
    "task3_pass = correct_task3 >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac68840",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GT3 Summary: Method Generalization\n",
      "======================================================================\n",
      "  Word Sentiment (Commonsense): 4/5 correct ✓ PASS\n",
      "  Adjective Antonym (Linguistic): 0/2 correct ✗ FAIL\n",
      "  Country Language (Factual): 2/3 correct ✓ PASS\n",
      "\n",
      "GT3 Result: PASS\n",
      "Rationale: LRE method successfully applies to multiple relation types (word sentiment, country language)\n"
     ]
    }
   ],
   "source": [
    "# Summary for GT3\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GT3 Summary: Method Generalization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gt3_results = [\n",
    "    {'task': 'Word Sentiment (Commonsense)', 'correct': correct_task1, 'total': 5, 'pass': task1_pass},\n",
    "    {'task': 'Adjective Antonym (Linguistic)', 'correct': correct_task2, 'total': min(5, len(test3_filtered.samples)), 'pass': task2_pass},\n",
    "    {'task': 'Country Language (Factual)', 'correct': correct_task3, 'total': min(5, len(test4_filtered.samples)), 'pass': task3_pass},\n",
    "]\n",
    "\n",
    "for result in gt3_results:\n",
    "    status = \"✓ PASS\" if result['pass'] else \"✗ FAIL\"\n",
    "    print(f\"  {result['task']}: {result['correct']}/{result['total']} correct {status}\")\n",
    "\n",
    "# GT3 passes if at least one task passes\n",
    "gt3_pass = any(r['pass'] for r in gt3_results)\n",
    "print(f'\\nGT3 Result: {\"PASS\" if gt3_pass else \"FAIL\"}')\n",
    "print(f'Rationale: {\"LRE method successfully applies to multiple relation types (word sentiment, country language)\" if gt3_pass else \"LRE method did not work on any tested relation types\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e1f69",
   "metadata": {},
   "source": [
    "## Summary: Generalizability Checklist\n",
    "\n",
    "| Criterion | Result | Evidence |\n",
    "|-----------|--------|----------|\n",
    "| **GT1: Model Generalization** | PASS | LRE works on GPT-2 Medium (2/3 correct, including South Korea → Seoul, United States → Washington) |\n",
    "| **GT2: Data Generalization** | PASS | LRE works on new countries not in original dataset (3/3: Poland→Warsaw, Sweden→Stockholm, Norway→Oslo) |\n",
    "| **GT3: Method Generalization** | PASS | LRE method applies to multiple relation types: word sentiment (4/5 = 80%), country language (2/3 = 67%) |\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Linear Relational Embedding (LRE) findings demonstrate strong generalizability:\n",
    "\n",
    "1. **Model Generalization**: The finding transfers to GPT-2 Medium, a model not used in the original study (which used GPT-J-6B, GPT-2-XL, LLaMA-13B).\n",
    "\n",
    "2. **Data Generalization**: The finding holds for new data instances (Poland, Sweden, Norway) not in the original 24-country dataset.\n",
    "\n",
    "3. **Method Generalization**: The LRE method can be applied to multiple similar tasks across different relation types (factual and commonsense relations). Note that some relations (like adjective antonym) do not work well, which is consistent with the paper's finding that not all relations are linearly decodable.\n",
    "\n",
    "All three generalizability criteria are satisfied with at least one successful example each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9cfa23",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation summary to: /net/scratch2/smallyan/relations_eval/evaluation/generalization_eval_summary.json\n",
      "\n",
      "Summary contents:\n",
      "{\n",
      "  \"Checklist\": {\n",
      "    \"GT1_ModelGeneralization\": \"PASS\",\n",
      "    \"GT2_DataGeneralization\": \"PASS\",\n",
      "    \"GT3_MethodGeneralization\": \"PASS\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"GT1_ModelGeneralization\": \"The LRE (Linear Relational Embedding) finding successfully transfers to GPT-2 Medium, a model not used in the original study (which used GPT-J-6B, GPT-2-XL, and LLaMA-13B). Testing the 'country capital city' relation on GPT-2 Medium achieved 2/3 correct predictions (66.67% faithfulness), with successful examples including 'South Korea \\u2192 Seoul' and 'United States \\u2192 Washington'. This demonstrates that the neuron-level linear approximation finding generalizes to a new model architecture/size.\",\n",
      "    \"GT2_DataGeneralization\": \"The LRE finding generalizes to new data instances not in the original dataset. Testing on countries not in the original 24-country dataset (Poland, Sweden, Norway), the LRE achieved 100% faithfulness (3/3 correct): Poland \\u2192 Warsaw, Sweden \\u2192 Stockholm, Norway \\u2192 Oslo were all correctly predicted. This demonstrates that the linear relational embedding generalizes beyond the training data.\",\n",
      "    \"GT3_MethodGeneralization\": \"The LRE method (Jacobian-based linear approximation) can be successfully applied to multiple similar tasks. Testing on different relation types: (1) Commonsense relations (word sentiment) achieved 80% faithfulness (4/5 correct predictions including blessed\\u2192positive, blissful\\u2192positive, cheerful\\u2192positive, delighted\\u2192positive). (2) Factual relations (country language) achieved 67% faithfulness (2/3 correct: South Korea\\u2192Korean, United States\\u2192English). While some relations like adjective antonym do not work well (as noted in the original paper - not all relations are linearly decodable), the method demonstrably applies to multiple task types.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation summary JSON\n",
    "import json\n",
    "import os\n",
    "\n",
    "eval_dir = '/net/scratch2/smallyan/relations_eval/evaluation'\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "# Build detailed rationales\n",
    "gt1_rationale = f\"\"\"The LRE (Linear Relational Embedding) finding successfully transfers to GPT-2 Medium, a model not used in the original study (which used GPT-J-6B, GPT-2-XL, and LLaMA-13B). Testing the 'country capital city' relation on GPT-2 Medium achieved 2/3 correct predictions (66.67% faithfulness), with successful examples including 'South Korea → Seoul' and 'United States → Washington'. This demonstrates that the neuron-level linear approximation finding generalizes to a new model architecture/size.\"\"\"\n",
    "\n",
    "gt2_rationale = f\"\"\"The LRE finding generalizes to new data instances not in the original dataset. Testing on countries not in the original 24-country dataset (Poland, Sweden, Norway), the LRE achieved 100% faithfulness (3/3 correct): Poland → Warsaw, Sweden → Stockholm, Norway → Oslo were all correctly predicted. This demonstrates that the linear relational embedding generalizes beyond the training data.\"\"\"\n",
    "\n",
    "gt3_rationale = f\"\"\"The LRE method (Jacobian-based linear approximation) can be successfully applied to multiple similar tasks. Testing on different relation types: (1) Commonsense relations (word sentiment) achieved 80% faithfulness (4/5 correct predictions including blessed→positive, blissful→positive, cheerful→positive, delighted→positive). (2) Factual relations (country language) achieved 67% faithfulness (2/3 correct: South Korea→Korean, United States→English). While some relations like adjective antonym do not work well (as noted in the original paper - not all relations are linearly decodable), the method demonstrably applies to multiple task types.\"\"\"\n",
    "\n",
    "summary = {\n",
    "    \"Checklist\": {\n",
    "        \"GT1_ModelGeneralization\": \"PASS\",\n",
    "        \"GT2_DataGeneralization\": \"PASS\",\n",
    "        \"GT3_MethodGeneralization\": \"PASS\"\n",
    "    },\n",
    "    \"Rationale\": {\n",
    "        \"GT1_ModelGeneralization\": gt1_rationale,\n",
    "        \"GT2_DataGeneralization\": gt2_rationale,\n",
    "        \"GT3_MethodGeneralization\": gt3_rationale\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(eval_dir, 'generalization_eval_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation summary to: {summary_path}\")\n",
    "print(\"\\nSummary contents:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de6e90d",
   "metadata": {
    "execution_status": "pending"
   },
   "outputs": [],
   "source": [
    "# Copy the current notebook to the evaluation directory\n",
    "import shutil\n",
    "\n",
    "# Current notebook path\n",
    "current_nb_path = '/home/smallyan/eval_agent/notebooks/2025-12-24-21-34_generalization_eval_v2.ipynb'\n",
    "target_nb_path = '/net/scratch2/smallyan/relations_eval/evaluation/generalization_eval.ipynb'\n",
    "\n",
    "# Copy the notebook\n",
    "shutil.copy(current_nb_path, target_nb_path)\n",
    "print(f\"Copied notebook to: {target_nb_path}\")\n",
    "\n",
    "# Verify it exists\n",
    "if os.path.exists(target_nb_path):\n",
    "    print(\"Verification: Notebook file exists ✓\")\n",
    "    nb_size = os.path.getsize(target_nb_path)\n",
    "    print(f\"File size: {nb_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-21-34_generalization_eval_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
